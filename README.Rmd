---
title: "Analysis of 2020-2022 Water Temperatures"
subtitle: "A Collaboration with STINAPA"
author: "Tadge McWilliams"
date: "June 8th, 2023"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

```{r loading packages, include=FALSE, echo=FALSE}
library(readxl)
library(tidyverse) 
library(lubridate) 
library(flextable) #for nice table
library(rempsyc) #for nice table
library(astsa) #for time series modeling and plotting
library(imputeTS)
```


```{r loading data, echo=FALSE, warning=FALSE}
setwd("~/Documents/Data Science/STINAPA/Temp & Light Project")
tl2020 <- read_excel("2020 raw.xlsx", sheet = 1)
tl2021 <- read_excel("2021 raw.xlsx", sheet = 1)
tl2022 <- read_excel("2022 raw.xlsx", sheet = 1)
```

![](/Users/tadgemcwilliams/Documents/Data Science/STINAPA/STINAPA/Joanne's Sunchi1.jpg)

## Summary
...summary of data, analysis, methods, and findings.

## Data Collected
...highlevel overview of the data collected.  Quantity, sites, depths, locations, missing data.

## Methodology
...how was data handeled. what formuli were used.  what past work was used

### Outliers

Outliers play an important part in this data set.  Statistically, outliers are observations that fall beyond the upper and lower quartiles by at least 1.5 times the interquartile range. While outliers can represent collection errors, they can also represent real environmental anomalies that lead to new discoveries or help to explain a greater trend in the data.  For these reasons, I have used a conservative approach in handling outliers.

For this data set I chose to include all observations below the first quartile.  My reasoning is as follows: First, it is reasonable to assume that abnormally low temperature readings are legitimate.  Given Bonaire's hot climate, unnatural readings are difficult to produce unless the sensor malfunctions or the sensor is placed in an artificially cooled environment.  Second, upwellings of colder water have been recorded in the past, and these upwellings can have a significant effect on the environment.  Thus, their presence is an important feature for study.

Outliers above the third quartile were carefully considered.  In reviewing the data, it is clear that some temperature readings are impossible ocean temperatures.  There is also a plausible scenario in which the data sensors continued to record temperatures when they were brought above water.  They were likely laying on a boat in direct sun during weekly cleaning and data retrieval.  I concluded that the impossible values should be removed from the dataset so as to yield a more accurate analysis, but choosing a threshold for unnatural observations was challenging.  Initially, I calculated the the threshold using a standard statistical outlier method (Q3 + 1.5 x IQR) for each year; however, a significant number of observations fall along that temperature.  Additionally, the data appears to have a multi-modal distribution.  This is likely due to seasonal weather patterns.  Setting a outlier threshold of 1.5 x IQR is typically only an acceptable practice with normally distributed data.

After consulting Erik Meesters, Tropical Marine Ecologist and Bio Statistician at Wageningen Marine Research, I agreed with his suggestion that any observations above 33 degrees celsius were not recorded in the water[^1].  The visualizations below help to confirm that while a 33 degree threshold is somewhat arbitrary, this higher threshold includes potentially natural outliers while excluding the majority of observations that are impossible for the climate.  This method could be deemed overly conservative or simplistic, but as Erik aptly suggested, it would be prudent not to omit any observations unless you are 100% sure the reading is wrong.

[^1]: E. Meesters, personal communication, March 16th, 2023.

```{r 2020 outliers, echo=FALSE, warning=FALSE}
ggplot(tl2020, aes(temp_c)) +
  geom_bar(width = 0.1) + 
  facet_wrap(~depth_m) +
  xlab("Temperature (c)") +
  ylab("Observations") +
  ggtitle("2020 Distribution by Sensor Depth (m)")

quant2020 <- quantile(tl2020$temp_c, probs = c(0.25, 0.75))
statoutlier2020 <- (IQR(tl2020$temp_c) * 1.5) + quant2020[2]

ggplot(tl2020, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2020 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2020, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```


```{r 2021 outliers, echo=FALSE, warning=FALSE}
ggplot(tl2021, aes(temp_c)) +
  geom_bar(width = 0.1) + 
  facet_wrap(~depth_m) +
  xlab("Temperature (c)") +
  ylab("Data Points") +
  ggtitle("2021 Distribution by Sensor Depth (m)")

quant2021 <- quantile(tl2021$temp_c, probs = c(0.25, 0.75))
statoutlier2021 <- (IQR(tl2021$temp_c) * 1.5) + quant2021[2]

ggplot(tl2021, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2021 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2021, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```


```{r 2022 outliers, echo=FALSE, warning=FALSE}
ggplot(tl2022, aes(temp_c)) +
  geom_bar(width = 0.1) + 
  facet_wrap(~depth_m) +
  xlab("Temperature (c)") +
  ylab("Data Points") +
  ggtitle("2022 Distribution by Sensor Depth (m)")

quant2022 <- quantile(tl2022$temp_c, probs = c(0.25, 0.75))
statoutlier2022 <- (IQR(tl2022$temp_c) * 1.5) + quant2022[2]

ggplot(tl2022, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2022 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2022, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```

```{r removing outliers above 33 degrees, echo=FALSE}
tl2020_33 <- tl2020 %>% filter(temp_c <= 33)
tl2021_33 <- tl2021 %>% filter(temp_c <= 33)
tl2022_33 <- tl2022 %>% filter(temp_c <= 33)
```


```{r formatting date and time columns, echo=FALSE}
tl2020_33$date <- as.Date(tl2020_33$date)
tl2020_33$time <- strftime(tl2020_33$time, format="%H:%M:%S")

tl2021_33$date <- as.Date(tl2021_33$date)
tl2021_33$time <- strftime(tl2021_33$time, format="%H:%M:%S")

tl2022_33$date <- as.Date(tl2022_33$date)
tl2022_33$time <- strftime(tl2022_33$time, format="%H:%M:%S")
```

## EDA?
  Looks at summary stats?
  
## Data Analysis

I aggregated the data by date, then filtered for the daily minimum, maximum, and average temperatures.  
Average temperatures during 2022 were lower in many months in in 2020 or 2021.  ...La Ni$\tilde{n}$a info to back this up???

```{r daily min/max and avg water temp by day, echo=FALSE}
#2020
temp20mm <- tl2020_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#2021
temp21mm <- tl2021_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#2022
temp22mm <- tl2022_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#Combine all years
temp_mma <- temp20mm %>%
  bind_rows(temp21mm) %>%
  bind_rows(temp22mm) %>%
  mutate(year = year(date),
         month = month(date))

```


```{r visualizing general temperature trends, echo=FALSE}
ggplot(temp_mma, aes(month,  avg_temp, color = factor(year))) +
  geom_smooth(se=FALSE, span = 0.05) +
  scale_color_manual(name = "Year", 
                       labels = c("2020", "2021", "2022"),
                       values = c("deepskyblue1","darkviolet","darkblue"),
                       guide = guide_legend(reverse = TRUE)) +
    scale_x_continuous(breaks = 1:12) + 
    labs(x = "Month", y = "Temperature (c)") +
    ggtitle("Average of Water Temperature") 


ggplot(temp_mma, aes(date, avg_temp, color = factor(depth_m))) +
  geom_smooth(se=FALSE) +
  labs(x = "Time",
       y = "Temperature (c)",
       color = "Depth (m)",
       title = "Average Water Temperature")


ggplot(temp_mma, aes(factor(depth_m), avg_temp, color = site)) +
  geom_boxplot() +
  labs(x = "Depth (m)",
       y = "Temperature (c)",
       color = "Site",
       title = "Average Water Temperature")
```


Below is a summary of the data collected by location.  Of all the sites, STINAPA collected the most data from Klein Bonaire 1.  Of all the depths, the most data was collected at 12 meters.  Finally, the most data collected for a specific location (site and depth) was from Klein Bonaire 1 at 20 meters.

The histogram indicates that the data is multi-modal.  Also, no one site appears to be significantly skewed in temperature.  


```{r reviewing data by site and depth, echo=FALSE}

table(temp_mma$site, temp_mma$depth_m) 
table(temp_mma$site, temp_mma$depth_m) %>% rowSums() 
table(temp_mma$site, temp_mma$depth_m) %>% colSums()
### can these be formatted to plotted with nice_table?

temp_mma %>% filter(depth_m == 12) %>%
  ggplot(aes(avg_temp, fill = site)) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(vars(site)) + 
  labs(x = "Temperature (c)", 
       y = "Observations",
       fill = "Site")
```



Below are tables of the lowest temperatures for each year.  From the *Temperature Variations* plots above, it is apparent that 2020 and 2022 had isolated low temperature events while the recorded temperatures in October 2021 were more graduated.
It is interesting that the isolated low temperature events in 2022 were all recorded at six meters.  No significant low temperatures were recorded at lower depths.  

```{r review of low outliers, echo=FALSE}

# 2020 low temp outliers
temp20mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2020 Temperature Observations Below 23 Degrees (c)")

#2021 low temp outliers
temp21mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2021 Temperature Observations Below 23 Degrees (c)")
            
#2022 low temp outliers
temp22mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2022 Temperature Observations Below 23 Degrees (c)")
```

### Trend Modeling

I made the decision to use the data collected from 20 meters from for my time series model.  By isolating my model to one location and one depth, I hope to create a more accurate model than if I were to assume all the data collected followed the same trend, regardless of depth.  Additionally, the Harbour Village data has few outliers in either direction over the three years of data collection.

Below, is the distribution of temperatures.  Once the maximum, average, and minimum temperatures are separated, a clear bi-modal distribution is visible.  Variation in the QQ-Plot also confirms the data is not normally distributed.  

Future modeling includes a seasonality component to acknowledge the distribution. 

```{r model prep- filtering to 12m}
# Below, filtering 12 meters from all sites, then averaging each day's data.
temp_12m <- temp_mma %>% 
  filter(depth_m == 20) %>%
  group_by(date)

# Below, filtering for dates and avg. temps
avgTemp12m <- temp_12m %>%
  summarize(avg_temp = mean(avg_temp)) %>%
  arrange(date)


# Plotting distribution of stats.
temp_12m %>% pivot_longer('min_temp':'max_temp', 
                          names_to = "stat", 
                          values_to = "value") %>%
  ggplot(aes(value, fill = stat)) +
    geom_histogram(binwidth = 0.1, 
                   alpha = 0.5, 
                   position = "identity") +
    scale_fill_discrete(labels = c("Average", "Maximum", "Minimum")) +
    labs(x = "Temperature (c)", 
         y = "Observations", 
         fill = "Daily Temperature",
         title = "Distribution of Temperatures at 12 Meters") +
    theme(legend.position = c(0.85, 0.8))

# QQ plot.  
qqnorm(avgTemp12m$avg_temp, main = "QQ-plot")
qqline(avgTemp12m$avg_temp, col = "red")
```


An important factor in time series analysis is the continuity of time.  Using the statNA function from the *imputeTS* package, I determine that 197 observations are missing from the timeline.  This represents a significant proportion of the data set (19.7%).  Unfortunately, filtering the original data set for temperature records at 6 meters or 20 meters yields a greater percentage of missing observations.  In an effort to minimize 
The plots below indicate that the data is missing at random.  Imputation must be used to complete the dataset as reducing the site of dataset would remove a full year of seasonal trends.....

```{r modeling prep for NAs}
# Populating complete date series

avgTemp12_full <- avgTemp12m %>%
  mutate(date = as.Date(date)) %>%
  complete(date = seq.Date(min(date), max(date), by="day"))


# Reviewing NAs by missing days.
statsNA(avgTemp12_full$avg_temp)

ggplot(avgTemp12_full, aes(date, avg_temp)) +
  geom_line(color = "blue4") +
  labs(x="Date",
       y="Average Temperature (c)",
       title="Visualizing Missing Data")

```


In order to minimize the amount of imputation needed, I have aggregated the data into weekly temperature averages.  This reduces the percentage of missing data.  Additionally, the weekly segments yields a more manageable data set for seasonal differencing.

```{r Modeling Prep- Aggregating into weekly data}
#Aggregating into weekly averages to minimize imputation needs.
avgTemp12_full <- avgTemp12_full %>% 
  mutate(week = week(date),
         year = year(date))

avgTemp12_weekly <- avgTemp12_full %>% 
  select(year, week, avg_temp) %>%
  group_by(year, week) %>%
  mutate(wk_temp = mean(avg_temp, na.rm=TRUE)) %>%
  distinct(year, week, wk_temp)

#Validating all weeks are present
#table(avgTemp12_weekly$year, avgTemp12_weekly$week)


#Reviewing missing data from weekly averages
statsNA(avgTemp12_weekly$wk_temp)

timeseries <- ts(avgTemp12_weekly$wk_temp)

ggplot_na_distribution(timeseries) +
  labs(y="Temperature (c)")
```


There are various imputation strategies.  The use of random, mean, or last-observed-carried-forward imputation were ruled out due to the seasonal nature of the data.  Linear, seasonally split, and seasonally decomposed imputation are modeled below.  Linear modeling is the simplest method, but appears overly simplistic.  The seasonal split method separates the data into seasons before imputing missing values base on each season.  This method adds dimension, but it is apparent that some imputations are based on micro trends from surrounding years that diverge from the immediate slope.  This is a likely due to the smaller size of data set.  Seasonally decomposed imputation removes the seasonal trends of the data set, imputes values based on the static series, then reintroduces the seasonal trend.  When applied to the current data set, this method applies dimensionality while following a more direct path between actual values.


```{r model prep, imputing missing values}

#kalman <- na_kalman(timeseries, model = "auto.arima")
#locf <- na_locf(timeseries, na_remaining = "rev")
#ma <- na_ma(timeseries, k=4, weighting="exponential")
interpolation <- na_interpolation(timeseries, option = "linear")
seasplit <- na_seasplit(timeseries, algorithm = "ma", find_frequency = TRUE)
seadec <- na_seadec(timeseries, algorithm = "interpolation", find_frequency = TRUE)

#ggplot_na_imputations(timeseries, kalman)
#ggplot_na_imputations(timeseries, locf)
#ggplot_na_imputations(timeseries, ma)
ggplot_na_imputations(timeseries,interpolation, title = "Linear Interpolation")
ggplot_na_imputations(timeseries, seasplit, title = "Seasonally Split Interpolation")
ggplot_na_imputations(timeseries, seadec, title = "Seasonally Decomposed Interpolation")

```


Using ....Autoregressive Integrated Moving Average (ARIMA)
```{r time series model 1}

ts_complete <- seadec

# Below, ploting time series and de-trended time series.
par(mfrow = c(2,1))
plot(ts_complete)
plot(diff(diff(ts_complete)))

# Process
# 1. save diff(diff(timeseries), lag = 52) to remove trends and make stationary.
# 2. plot standardized object.
# 3. run acf2() to compare lag drop offs or tails between acf vs pacf(). 
#.      tailing acf suggests AR model (x,1,0,0); tailing pacf suggests MA model (x,0,0,1.  both suggests ARMA model (x,1,0,1)

# Below, plotting auto correlation and partial auto correlation.


acf2(diff(diff(ts_complete)),  max.lag=20)


# Use sarima() from astsa package for multi-plot of ts model.

#sarima(ts_complete, p,d,q, P,D,Q, S)
model <- sarima(ts_complete, p=0,d=1,q=0,P=0,D=1,Q=1,S=53, max.lag = 52)
model
model$ttable

# Forcasting
# use arima.for(ts_complete, n.head = number of periods to forcase, 1,1,1). Last args match model params.
```


