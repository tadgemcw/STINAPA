---
title: "Analysis of Water Temperature Trends With ARIMA"
subtitle: "A Collaboration with STINAPA"
author: "Tadge McWilliams"
date: "July 30th, 2023"
output: 
  html_document:
    toc: TRUE
    toc_float:
      collapsed: FALSE
---

```{r loading packages, echo=FALSE}
library(knitr) #global settings
library(readxl)
library(tidyverse) 
library(lubridate) 
library(flextable) #for nice table
library(rempsyc) #for nice table
library(astsa) #for time series modeling and plotting
library(imputeTS) #for imputing missing time series data
library(forecast) #for auto.arima
```

```{r global settings, setup, include=FALSE}
opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


```{r loading data}
setwd("~/Documents/Data Science/STINAPA/Temp & Light Project")
tl2020 <- read_excel("2020 raw.xlsx", sheet = 1)
tl2021 <- read_excel("2021 raw.xlsx", sheet = 1)
tl2022 <- read_excel("2022 raw.xlsx", sheet = 1)
```

![](/Users/tadgemcwilliams/Documents/Data Science/STINAPA/STINAPA/Joanne's Sunchi1.jpg)

## Summary
...summary of data, analysis, methods, and findings.

## Data Collected
The data provided was collected during 2020 through 2022 by Stichting Nationale Parken Bonaire (STINAPA) using HOBO temperature and light data loggers.  Data loggers were place in four locations and at three depths per location.  Each data logger recorded a temperature observation every five minutes.  Each weeks, data was collected and the HOBO data loggers were cleaned by STINAPA. In total, there are **1,932,026 observations**.  Each observation was recorded with the following variables:

* **date:** Date observation was collected (Y-M-D).
* **time:** Time observation was collected (H-M-S).
* **temp_c:** Temperature recorded in celcius.
* **sensor_id:** Serial number of HOBO data logger. 
* **site:** Location of datalogger.
* **depth_m:** The depth of the data logger in meters.

## Methodology
...how was data handeled. what formuli were used.  what past work was used

### Data Cleaning

Outliers play an important part in this data set.  Statistically, outliers are observations that fall beyond the upper and lower quartiles by at least 1.5 times the interquartile range. While outliers can represent collection errors, they can also represent real environmental anomalies that lead to new discoveries or help to explain a greater trend in the data.  For these reasons, I have used a conservative approach in handling outliers.

For this data set I chose to include all observations below the first quartile.  My reasoning is as follows: First, it is reasonable to assume that abnormally low temperature readings are legitimate.  Given Bonaire's hot climate, unnatural readings are difficult to produce unless a data logger malfunctions or is placed in an artificially cooled environment.  Secondly, upwellings of colder water have been recorded in the past, and these upwellings can have a significant effect on the environment.  Thus, their presence is an important feature for study.

Outliers above the third quartile were carefully considered.  In reviewing the data, it is clear that some temperature readings are impossible ocean temperatures.  There is also a plausible scenario in which the data loggers continued to record temperatures when they were brought above water.  They were likely placed on a boat in direct sunlight during weekly cleaning and data retrieval.  I concluded that the impossible values should be removed from the data set so as to yield a more accurate analysis, but choosing a threshold for unnatural observations was challenging.  Initially, I calculated the the threshold using a standard statistical outlier method (Q3 + 1.5 x IQR) for each year; however, a significant number of observations fall along that threshold.  Additionally, the data appears to have a bi-modal distribution due to seasonal weather patterns.  Using an outlier threshold of 1.5 x IQR is typically only an acceptable practice with normally distributed data.

In consulting Erik Meesters, Tropical Marine Ecologist and Bio Statistician at Wageningen Marine Research, I agreed with his suggestion that any observations above 33 degrees celsius were not recorded in the water[^1].  The visualizations below help to confirm that while a 33 degree threshold is made a priori, the greater threshold includes potentially natural outliers while excluding the majority of observations that are impossible for the climate.  This method could be deemed overly conservative or simplistic, but as Erik aptly suggested, it would be prudent not to omit any observations unless you are 100% sure the reading is wrong.

[^1]: E. Meesters, personal communication, March 16th, 2023.

```{r 2020 outliers}
#ggplot(tl2020, aes(temp_c)) +
#  geom_bar(width = 0.1) + 
#  facet_wrap(~depth_m) +
#  xlab("Temperature (c)") +
#  ylab("Observations") +
#  ggtitle("2020 Distribution by Sensor Depth (m)")

quant2020 <- quantile(tl2020$temp_c, probs = c(0.25, 0.75))
statoutlier2020 <- (IQR(tl2020$temp_c) * 1.5) + quant2020[2]

ggplot(tl2020, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2020 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2020, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```


```{r 2021 outliers}
#ggplot(tl2021, aes(temp_c)) +
#  geom_bar(width = 0.1) + 
#  facet_wrap(~depth_m) +
#  xlab("Temperature (c)") +
#  ylab("Data Points") +
#  ggtitle("2021 Distribution by Sensor Depth (m)")

quant2021 <- quantile(tl2021$temp_c, probs = c(0.25, 0.75))
statoutlier2021 <- (IQR(tl2021$temp_c) * 1.5) + quant2021[2]

ggplot(tl2021, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2021 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2021, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```


```{r 2022 outliers}
#ggplot(tl2022, aes(temp_c)) +
#  geom_bar(width = 0.1) + 
#  facet_wrap(~depth_m) +
#  xlab("Temperature (c)") +
#  ylab("Data Points") +
#  ggtitle("2022 Distribution by Sensor Depth (m)")

quant2022 <- quantile(tl2022$temp_c, probs = c(0.25, 0.75))
statoutlier2022 <- (IQR(tl2022$temp_c) * 1.5) + quant2022[2]

ggplot(tl2022, aes(x=factor(month(date)), y = temp_c)) + 
    stat_boxplot(geom ='errorbar', width = 0.2) + 
    geom_boxplot() +
    xlab("Month") + 
    ylab("Water Temperature (c)") +
    ggtitle("2022 Temperature Variations") +
    geom_hline(aes(yintercept = 33, col = "33 Degrees")) + 
    geom_hline(aes(yintercept = statoutlier2022, col = "1.5 x IQR")) +
    theme(legend.title=element_blank())
```

```{r removing outliers above 33 degrees}
tl2020_33 <- tl2020 %>% filter(temp_c <= 33)
tl2021_33 <- tl2021 %>% filter(temp_c <= 33)
tl2022_33 <- tl2022 %>% filter(temp_c <= 33)
```


```{r formatting date and time columns}
tl2020_33$date <- as.Date(tl2020_33$date)
tl2020_33$time <- strftime(tl2020_33$time, format="%H:%M:%S")

tl2021_33$date <- as.Date(tl2021_33$date)
tl2021_33$time <- strftime(tl2021_33$time, format="%H:%M:%S")

tl2022_33$date <- as.Date(tl2022_33$date)
tl2022_33$time <- strftime(tl2022_33$time, format="%H:%M:%S")
```

  
## Data Analysis

### Summary Statistics

I aggregated the data by date, then filtered for the daily minimum, maximum, and average temperatures.  
Average temperatures during 2021 were lower in many months in in 2020 or 2022.  Review of average temperatures by depth indicate a significant decrease in temperature at six meters during 2021 and a greater spread between depths in 2022.  

```{r daily min/max and avg water temp by day}
#2020
temp20mm <- tl2020_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#2021
temp21mm <- tl2021_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#2022
temp22mm <- tl2022_33 %>% 
  select(date, site, depth_m, temp_c) %>%
  group_by(site, depth_m, date) %>% 
  summarise(min_temp = min(temp_c),
            avg_temp = mean(temp_c),
            max_temp = max(temp_c)) %>%
  arrange(date, site, depth_m)

#Combine all years
temp_mma <- temp20mm %>%
  bind_rows(temp21mm) %>%
  bind_rows(temp22mm) %>%
  mutate(year = year(date),
         month = month(date))

# pander::pander(summary(temp_mma))
```


```{r visualizing general temperature trends}
ggplot(temp_mma, aes(month,  avg_temp, color = factor(year))) +
  geom_smooth(se=FALSE, span = 0.05) +
  scale_color_manual(name = "Year", 
                       labels = c("2020", "2021", "2022"),
                       values = c("deepskyblue1","darkviolet","darkblue"),
                       guide = guide_legend(reverse = TRUE)) +
    scale_x_continuous(breaks = 1:12) + 
    labs(x = "Month", y = "Temperature (c)") +
    ggtitle("Average Water Temperature by Year") 


ggplot(temp_mma, aes(date, avg_temp, color = factor(depth_m))) +
  geom_smooth(se=FALSE) +
  labs(x = "Year",
       y = "Temperature (c)",
       color = "Depth (m)",
       title = "Average Water Temperature by Depth")


ggplot(temp_mma, aes(factor(depth_m), avg_temp, color = site)) +
  geom_boxplot() +
  labs(x = "Depth (m)",
       y = "Temperature (c)",
       color = "Site",
       title = "Average Water Temperature by Depth & Location")
```


Below is a summary of the data collected by location.  Of all the sites, STINAPA collected the most data from Klein Bonaire 1.  Of all the depths, the most data was collected at 12 meters.  Finally, the most data collected for a specific location (site and depth) was from Klein Bonaire 1 at 20 meters.

The histogram indicates that the data is multi-modal.  This is logical as the observations reflect seasonal data.  Changes between summer and winter create a multi-modal distribution of temperatures.  


```{r reviewing data by site and depth}

table(temp_mma$site, temp_mma$depth_m) 
table(temp_mma$site, temp_mma$depth_m) %>% rowSums() 
table(temp_mma$site, temp_mma$depth_m) %>% colSums()
### can these be formatted to plotted with nice_table?

temp_mma %>% filter(depth_m == 12) %>%
  ggplot(aes(avg_temp, fill = site)) +
  geom_histogram(binwidth = 0.1) +
  facet_wrap(vars(site)) + 
  labs(x = "Temperature (c)", 
       y = "Observations",
       fill = "Site",
       title = "Observations Collected by Temperature & Location")
```

### Low Temperature Outliers

The plots above indicate a number of observations with temperatures below the first quartile.  The tables below list the lowest temperature observations for each year. It is notable that the isolated low temperature events in 2022 were all recorded at six meters.  No significant low temperatures were recorded at lower depths.  

```{r review of low outliers}

# 2020 low temp outliers
temp20mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2020 Temperature Observations Below 23 Degrees (c)")

#2021 low temp outliers
temp21mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2021 Temperature Observations Below 23 Degrees (c)")
            
#2022 low temp outliers
temp22mm %>% filter(min_temp < 23) %>% arrange(date)  %>%
  rename("Date" = date, 
         "Site" = site, 
         "Depth (m)" = depth_m, 
         "Minimum" = min_temp, 
         "Average" = avg_temp,
         "Maximum" = max_temp) %>%
  nice_table(title = "2022 Temperature Observations Below 23 Degrees (c)")
```

## Model Developement

I made the decision to use the data collected from 20 meters from for the time series model.  By isolating the model to one depth, I hope to create a more accurate model than if I were to assume all the data followed one trend, regardless of depth.  

Below is the distribution of temperatures.  Once the maximum, average, and minimum temperatures are separated, a clear bi-modal distribution is visible.  Variation in the QQ-Plot also confirms the data is not normally distributed.  

Future modeling must include a seasonality component to account for the distribution. 

```{r model prep- filtering to 12m}
# Below, filtering 12 meters from all sites, then averaging each day's data.
temp_20m <- temp_mma %>% 
  filter(depth_m == 20) %>%
  group_by(date)

       
# Below, filtering for dates and avg. temps
avgTemp20m <- temp_20m %>%
  summarize(avg_temp = mean(avg_temp)) %>%
  arrange(date)


# Plotting distribution of stats.
temp_20m %>% pivot_longer('min_temp':'max_temp', 
                          names_to = "stat", 
                          values_to = "value") %>%
  ggplot(aes(value, fill = stat)) +
    geom_histogram(binwidth = 0.1, 
                   alpha = 0.5, 
                   position = "identity") +
    scale_fill_discrete(labels = c("Average", "Maximum", "Minimum")) +
    labs(x = "Temperature (c)", 
         y = "Observations", 
         fill = "Daily Temperature",
         title = "Distribution of Temperatures at 20 Meters") +
    theme(legend.position = c(0.85, 0.8))

# QQ plot.  
qqnorm(avgTemp20m$avg_temp, main = "QQ-plot")
qqline(avgTemp20m$avg_temp, col = "red")
```
### Missing Data

An important factor in time series analysis is the continuity of time.  Using the statNA function from the *imputeTS* package, I determine that 220 observations (days) are missing from the timeline.  This represents a significant proportion of the data set (22%).  
The plots below indicate that the data is missing at random.  Imputation must be used to complete the data set as reducing the length of data set would not yield a complete year by which to model seasonal trends.

```{r modeling prep for NAs}
# Populating complete date series

avgTemp20_full <- avgTemp20m %>%
  mutate(date = as.Date(date)) %>%
  complete(date = seq.Date(min(date), max(date), by="day"))


# Reviewing NAs by missing days.
statsNA(avgTemp20_full$avg_temp)

```


In order to minimize the amount of imputation needed, I aggregate the data into weekly temperature averages.  This reduces the percentage of missing data to 15.2% (22 weeks).  Additionally, the weekly averages yield a more manageable data set for future seasonal differencing.  Below is a plot of missing data by week.

```{r Modeling Prep- Aggregating into weekly data}
#Aggregating into weekly averages to minimize imputation needs.
avgTemp20_full <- avgTemp20_full %>% 
  mutate(week = week(date),
         year = year(date))

avgTemp20_weekly <- avgTemp20_full %>% 
  select(year, week, avg_temp) %>%
  group_by(year, week) %>%
  mutate(wk_temp = mean(avg_temp, na.rm=TRUE)) %>%
  distinct(year, week, wk_temp)

#Validating all weeks are present
#table(avgTemp12_weekly$year, avgTemp12_weekly$week)


#Reviewing missing data from weekly averages
statsNA(avgTemp20_weekly$wk_temp)

timeseries <- ts(avgTemp20_weekly$wk_temp)

ggplot_na_distribution(timeseries) +
  labs(x="Weeks", y="Temperature (c)", subtitle = "Weekly averages with highlighted missing regions")
```

### Imputation

There are various imputation strategies.  The use of random, mean, or last-observed-carried-forward imputation were ruled out due to the seasonal nature of the data.  Seasonally Split, and Seasonally Decomposed imputation methods are provided below.  

The Seasonally Split method separates the data into seasons before imputing missing values for each season.  This method adds dimension, but it is apparent that some imputations are based on micro trends of adjacent years and diverge from the seasonal slope.  The Seasonally Decomposed imputation method removes the seasonal trends of the data set, imputes values based on the de-trended static time series, then reintroduces the seasonal trends.  When applied to the data set, this method applies dimensionality while following a more direct slope between known values than the seasonal split method.  Therefore, I selected Seasonally Decomposed imputation for this model.


```{r model prep, imputing missing values}

#kalman <- na_kalman(timeseries, model = "auto.arima")
#locf <- na_locf(timeseries, na_remaining = "rev")
#ma <- na_ma(timeseries, k=4, weighting="exponential")
#interpolation <- na_interpolation(timeseries, option = "linear")
seasplit <- na_seasplit(timeseries, algorithm = "ma", find_frequency = TRUE)
seadec <- na_seadec(timeseries, algorithm = "interpolation", find_frequency = TRUE)

#ggplot_na_imputations(timeseries, kalman)
#ggplot_na_imputations(timeseries, locf)
#ggplot_na_imputations(timeseries, ma)
#ggplot_na_imputations(timeseries,interpolation, title = "Linear Interpolation")
ggplot_na_imputations(timeseries, seasplit, title = "Seasonally Split Interpolation")
ggplot_na_imputations(timeseries, seadec, title = "Seasonally Decomposed Interpolation")

```

### ARIMA Model Tuning

To model time series trends, I use a Autoregressive Integrated Moving Average (ARIMA) model. After differencing the time series to remove trends with a 52 week lag, I apply an auto-correlation function to view the correlations by time series lag.  The tailing ACF and dropoff in Partial ACF suggest that the ARIMA model will include a autoregressive component.  The presence of the Partial ACF correlation at lag 12 suggests that the model should include a moving average component as well. 

```{r time series model 1}
ts_complete <- seadec

# Below, ploting time series and de-trended time series.
par(mfrow = c(3,1))
plot(ts_complete)
plot(diff(ts_complete, lag = 52))
plot(diff(diff(ts_complete, lag = 52)))

# Process
# 1. save diff(diff(timeseries)) to remove trends and make stationary.
# 2. plot standardized object.
# 3. run acf2() to compare lag drop offs or tails between acf vs pacf(). 
#      tailing acf suggests AR model (x,1,0,0); tailing pacf suggests MA model (x,0,0,1.  both suggests ARMA model (x,1,0,1)

# Below, plotting auto correlation and partial auto correlation.
ts_complete %>% diff(lag=52) %>% acf2(max.lag = 52)

```

I calculate a basic ARIMA model using the *auto.arima()* function.  In agreement with the ACF calculations, the suggested model is an ARIMA(2,0,1) model with two autoregressive components and one moving average component.  The p-value for all components is zero.  Also, the plot suggests that the residual values follow a white noise pattern, indicating a well fitting model.

Surprisingly, the auto.arima() function did not select a seasonal component.  Below the ARIMA(2,0,1) model results, I manually fit a seasonal ARIMA model (SARIMA) in an effort to find a more efficient model.  While a SARIMA(2,0,1)(0,0,1){26} model comes quite close in AIC to the ARIMA(2,0,1) model, the ARIMA model is marginally more accurate on this data set.  It is possible that annual trends are not significant in this data set due to the number of years included in the data set.  The ARIMA(2,0,1) model is most significant for this data set; however, it would be wise to reconsider the SARIMA(2,0,1)(0,0,1){26} model if data set were expanded to include additional years. 


```{r modeling and tuning}
# Use sarima() from astsa package for multi-plot of ts model.
#sarima(ts_complete, p,d,q, P,D,Q, S)

# Non-seasonal model with auto.arima from forecast package.
auto.arima(ts_complete) 
sarima(ts_complete, p=2, d=0, q=1)

# seasonal model
sarima(ts_complete, p=2,d=0,q=1,P=0,D=0,Q=1,S=26)

```

### Model Predictions

Below, the both models are trained on 90% of the time series then used to predict the remaining 10% of the known temperature values.  When the estimated values are compared with the actual values, the both models appear to fit well.  Additionally, the seasonal model has a marginally smaller root-mean-square-error (RMSE) than the non-seasonal model.


```{r arima model predictions}

#visualizing ARIMA(2,0,1) accuracy
train  <- window(ts_complete, end = round(length(ts_complete)*0.9, 0)) # 90% of data
test <- window(ts_complete, end = length(ts_complete))
new_values <- 52
pred <- sarima.for(train, n.ahead = new_values, p=2,d=0,q=1, 
                   main="ARIMA(2,0,1) Temperature Predictions",
                   xlab="Weeks")
lines(test)

#RMSE sqrt(mean((pred$pred - test)^2))
rmse <- sqrt(mean((pred$pred[1:(length(test) - length(train))] - tail(test, n=length(test) - length(train)))^2))
print(paste("The RMSE for this ARIMA(2,0,1) model is ", round(rmse, 4)))



#visualizing accuracy of SARIMA model
strain  <- window(ts_complete, end = round(length(ts_complete)*0.9, 0)) # 90% of data
stest <- window(ts_complete, end = length(ts_complete))
new_values <- 52
spred <- sarima.for(strain, n.ahead = new_values, p=2,d=0,q=1,P=0,D=0,Q=1,S=26, 
                   main="SARIMA(2,0,1)(0,0,1)[26] Temperature Predictions",
                   xlab="Weeks")
lines(test)

#RMSE sqrt(mean((pred$pred - test)^2))
srmse <- sqrt(mean((spred$pred[1:(length(stest) - length(strain))] - tail(stest, n=length(stest) - length(strain)))^2))
print(paste("The RMSE for this SARIMA(2,0,1)(0,0,1)[26] model is ", round(srmse, 4)))
```

